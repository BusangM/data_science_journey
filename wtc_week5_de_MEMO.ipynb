{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BusangM/data_science_journey/blob/main/wtc_week5_de_MEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A504hqzPBBB"
      },
      "source": [
        "# Data Eng: Week 5 Practical Memo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "First, you need to set up your environment, which includes installing `apache-beam` and downloading a text file from Cloud Storage to your local file system. We are using this file to test your pipeline."
      ],
      "metadata": {
        "id": "dBBmUcApM8P_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run and print a shell command.\n",
        "def run(cmd):\n",
        "  print('>> {}'.format(cmd))\n",
        "  !{cmd}\n",
        "  print('')\n",
        "\n",
        "run('pip install --upgrade pip')\n",
        "\n",
        "# Install apache-beam.\n",
        "run('pip install --quiet apache-beam')\n",
        "\n",
        "# Copy the input file into the local file system.\n",
        "run('wget https://storage.googleapis.com/bdt-beam-store/orders.csv -O orders.csv')\n",
        "run('wget https://storage.googleapis.com/bdt-beam-store/users.csv -O users.csv')\n"
      ],
      "metadata": {
        "id": "fKUs8cSkM-ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "import re\n",
        "\n",
        "outputs_prefix = 'outputs/part'"
      ],
      "metadata": {
        "id": "SjKVpsTVNG_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Basic transformations\n",
        "\n",
        "### Task 1.1\n",
        "> Read the input file (users.csv) into an initial PCollection\n",
        "\n",
        "### Task 1.2\n",
        "> Perform a transform to split each row from the input file into separate elements (e.g. user, gender etc.) so that you may process them"
      ],
      "metadata": {
        "id": "QZPf7KEj_6nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_header = ['user', 'gender', 'age', 'address', 'date']\n",
        "\n",
        "\n",
        "class Transform(beam.DoFn):\n",
        "\n",
        "  # Use classes to perform transformations on your PCollections\n",
        "  # Yield or return the element(s) needed as input for the next transform\n",
        "  def process(self, element):\n",
        "    yield dict(zip(user_header, element.split(',')))\n",
        "\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText('users.csv')\n",
        "      | 'format line' >> beam.ParDo(Transform())\n",
        "      | \"print\" >> beam.Map(print)\n",
        "  )\n"
      ],
      "metadata": {
        "id": "4DUAyg2uAhHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Variations on function\n",
        "\n",
        "Possible variations include the options below:"
      ],
      "metadata": {
        "id": "OfgTFJ1jNhIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_one(x):  # note, this is not a class, so we omit the `self` reference\n",
        "  \"\"\"\n",
        "  :return: a dictionary of key-value pairs\n",
        "  \"\"\"\n",
        "  x = x.split(',')\n",
        "  return {'user': x[0], 'gender': x[1], 'age': x[2], 'address': x[3], 'date': x[4]}\n",
        "\n",
        "\n",
        "def process_two(x):\n",
        "  \"\"\"\n",
        "  :return: a list of values (it is ordered so you can just keep track of the indices\n",
        "  through your code  and you are good)\n",
        "  \"\"\"\n",
        "  return x.split(',')\n",
        "\n",
        "\n",
        "print(process_one(\"Amy Sullivan,Female,20, Westlake-OH-44145,2020/08/31\"))\n",
        "print(process_two(\"Amy Sullivan,Female,20, Westlake-OH-44145,2020/08/31\"))\n"
      ],
      "metadata": {
        "id": "PWeLDXyzAogN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.3\n",
        "\n",
        ">  Perform a transform to change the date format as required"
      ],
      "metadata": {
        "id": "2NxOqDDgNw-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A Correct Answer\n",
        "\n",
        "We simply modify the function in the transform class (namely `process`), or we may add another function.\n",
        "\n",
        "It is pleasant to isolate functionality and responsibility to functions, so we will take that route.\n",
        "\n",
        "We are also going to use the python `datetime` class as it is best practice (it will fail if a date string is invalid thus alerting you to issues in your data). However, you could have used simple string replacement; this would just be a little less robust/easy to debug in the event of failure.\n",
        "\n",
        "Using the datetime approach:\n",
        "```\n",
        "datetime.strptime('2020-02-01', '%Y-%m-%d')\n",
        "```\n",
        "produces an object\n",
        "```\n",
        "datetime.datetime(2020, 2, 1, 0, 0)\n",
        "```\n",
        "\n",
        "Otherwise, you will get a nice error saying your date is in the incorrect format.\n",
        "\n",
        "We will then convert it back into a string (crazy, yes, it may seem so, but with dirtier data, this could be handy).\n",
        "\n",
        "```\n",
        "datetime.strptime('2020-02-01', '%Y-%m-%d').strftime('%Y/%m/%d')\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "FxqgvllGN8z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "user_header = ['User', 'Gender', 'Age', 'Address', 'Date joined']\n",
        "\n",
        "\n",
        "class Transform(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    data = dict(zip(user_header, element.split(',')))\n",
        "    # if the header is included in the pcollection, simply return it else we get\n",
        "    # a failure on our date conversion\n",
        "    if list(data.values()) == user_header:\n",
        "      yield data\n",
        "    else:\n",
        "      data['Date joined'] = self.format_date(data['Date joined'])\n",
        "      yield data\n",
        "\n",
        "\n",
        "  def format_date(self, value):\n",
        "    \"\"\"\n",
        "    :param value: receive a value that we assume is a date\n",
        "    :return: a format validated formatted date string\n",
        "    \"\"\"\n",
        "    return datetime.strptime(value, '%Y/%m/%d').strftime('%Y-%m-%d')\n",
        "\n",
        "\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText('users.csv')\n",
        "      | 'format line' >> beam.ParDo(Transform())\n",
        "      | \"print\" >> beam.Map(print)\n",
        "  )\n"
      ],
      "metadata": {
        "id": "asU4FAJGNzsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Variations on Function\n",
        "\n",
        "We could have specified the `format_date` function differently. Interestingly, you would not have noticed when parsing the header of the file since `value.replace('-', '/')` would simply make no modifications to the string.\n",
        "\n"
      ],
      "metadata": {
        "id": "TK8sNbnWOBOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_date(value):\n",
        "  return value.replace('-', '/')\n",
        "\n",
        "format_date('2020-02-01')"
      ],
      "metadata": {
        "id": "gI7ag4tCODom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.4\n",
        "\n",
        "> Perform a transform to change the address format as required\n",
        "\n"
      ],
      "metadata": {
        "id": "o-VPlxZgOIDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "user_header = ['User', 'Gender', 'Age', 'Address', 'Date joined']\n",
        "\n",
        "\n",
        "class Transform(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    data = dict(zip(user_header, element.split(',')))\n",
        "    # if the header is included in the pcollection, simply return it else we get\n",
        "    # a failure on our date conversion\n",
        "    if list(data.values()) == user_header:\n",
        "      yield data\n",
        "    else:\n",
        "      data['Date joined'] = self.format_date(data['Date joined'])\n",
        "      data['Address'] = self.format_address(data['Address'])\n",
        "      yield data\n",
        "\n",
        "\n",
        "  def format_date(self, value):\n",
        "    \"\"\"\n",
        "    :param value: receive a value that we assume is a date\n",
        "    :return: a format validated formatted date string\n",
        "    \"\"\"\n",
        "    return datetime.strptime(value, '%Y/%m/%d').strftime('%Y-%m-%d')\n",
        "\n",
        "  def format_address(self, value):\n",
        "    \"\"\"\n",
        "    perform necessary operations on address to get it into the correct format\n",
        "    \"\"\"\n",
        "    # noticed that there is a space before the address, so cleaning that\n",
        "    # up too\n",
        "    return value.replace('-', ',').strip(' ')\n",
        "\n",
        "\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText('users.csv')\n",
        "      | 'format line' >> beam.ParDo(Transform())\n",
        "      | \"print\" >> beam.Map(print)\n",
        "  )\n"
      ],
      "metadata": {
        "id": "0JPrGK-qOJfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Answer\n",
        "\n",
        "### Long answer"
      ],
      "metadata": {
        "id": "SQ1wMRhrOMCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "user_header = ['User', 'Gender', 'Age', 'Address', 'Date joined']\n",
        "\n",
        "\n",
        "class Transform(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    data = dict(zip(user_header, element.split(',')))\n",
        "    # if the header is included in the pcollection, simply return it else we get\n",
        "    # a failure on our date conversion\n",
        "    if list(data.values()) == user_header:\n",
        "      yield data\n",
        "    else:\n",
        "      data['Date joined'] = self.format_date(data['Date joined'])\n",
        "      data['Address'] = self.format_address(data['Address'])\n",
        "      yield data\n",
        "\n",
        "\n",
        "  def format_date(self, value):\n",
        "    \"\"\"\n",
        "    :param value: receive a value that we assume is a date\n",
        "    :return: a format validated formatted date string\n",
        "    \"\"\"\n",
        "    return datetime.strptime(value, '%Y/%m/%d').strftime('%Y-%m-%d')\n",
        "\n",
        "  def format_address(self, value):\n",
        "    \"\"\"\n",
        "    perform necessary operations on address to get it into the correct format\n",
        "    \"\"\"\n",
        "    # noticed that there is a space before the address, so cleaning that\n",
        "    # up too\n",
        "    return value.replace('-', ',').strip(' ')\n",
        "\n",
        "\n",
        "class ToCsv(beam.DoFn):\n",
        "\n",
        "    def process(self, element):\n",
        "      yield ','.join(dict(element).values())\n",
        "\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText('users.csv')\n",
        "      | 'format line' >> beam.ParDo(Transform())\n",
        "      | 'to_csv' >> beam.ParDo(ToCsv())\n",
        "      | \"print\" >> beam.Map(print) #beam.io.WriteToText(outputs_prefix)\n",
        "  )"
      ],
      "metadata": {
        "id": "kRDV5_EFOO1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Short answer"
      ],
      "metadata": {
        "id": "zfHUnRniOSQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Transform(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    row_list =  element.split(',')\n",
        "    row_list[4] = row_list[4].replace('/', '-')\n",
        "    row_list[3] = row_list[3].replace('-', ',')\n",
        "    yield ','.join(row_list)\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText('users.csv')\n",
        "      | 'format line' >> beam.ParDo(Transform())\n",
        "      | \"print\" >> beam.Map(print) #beam.io.WriteToText(outputs_prefix)\n",
        "  )"
      ],
      "metadata": {
        "id": "o3tZ_-gXOS6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Aggregations\n",
        "\n",
        "### Task 2.1\n",
        "> Perform a transform to determine the % split between female and male customers\n",
        "\n",
        "Here we follow a map-reduce style workload. We emit tuples of (M/F, 1), where (Male, 1) is emitted if male and (Female, 1) is emitted if female.\n",
        "\n",
        "Test you code with\n",
        "```\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText('users.csv')\n",
        "      | 'format line' >> beam.ParDo(Transform())\n",
        "      | 'add_key' >> beam.Map(lambda elem: (elem[1], 1))  # emit (M/F, 1) pairs\n",
        "      | \"print\" >> beam.Map(print)\n",
        "  )\n",
        "```\n",
        "\n",
        "and by removing the `combineByKey` part.\n",
        "\n",
        "We are defining a new `Transform` function here because we want to get rid of that header."
      ],
      "metadata": {
        "id": "jHAkk2noOXBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_header = ['User', 'Gender', 'Age', 'Address', 'Date joined']\n",
        "\n",
        "class Transform(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    row_list =  element.split(',')\n",
        "    if len(set(user_header).symmetric_difference(set(row_list))) == 0:\n",
        "      pass\n",
        "    else:\n",
        "      row_list[4] = row_list[4].replace('/', '-')\n",
        "      row_list[3] = row_list[3].replace('-', ',')\n",
        "      yield row_list\n",
        "\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText('users.csv')\n",
        "      | 'format line' >> beam.ParDo(Transform())\n",
        "      | 'add_key' >> beam.Map(lambda elem: (elem[1], 1))  # emit (M/F, 1) pairs\n",
        "      | 'sum' >> beam.CombinePerKey(sum)\n",
        "      | \"print\" >> beam.Map(print)\n",
        "  )\n"
      ],
      "metadata": {
        "id": "UAt5RmznOa_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Variations on function\n",
        "There are many ways to do it, but to do it in a beam-stream like fashion (remember, we are working with bounded data here, but it is worthwhile exploring the tool) is a little trickier.\n",
        "\n",
        "If we want to keep it properly within Beam, i.e. we don't extract the count and genders from each PCollection, but rather create a directed_acyclic_graph (DAG) that solves our problem, then see the Example 5: Combining with side inputs as singletons functionality.\n",
        "\n",
        "A singleton is a object oriented programming concept whereby an object is created once, and subsequent calls to that object in the code will access that same object.\n",
        "\n",
        "Interesting, and you will recall from the conversation about Spark, these workloads are designed for massive parallel computation. This means that PCollections are split over multiple machines and aren't made for collecting values from arbitrarily (see SO)\n",
        "\n",
        "This following cell would have been sufficient to answer the question. It is not entirely neat as we are not calculating the percentages directly from the pipelines, but that will be done in the next more complete answer."
      ],
      "metadata": {
        "id": "sDl_HYLiQsgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "\n",
        "  cleaned_csv = (pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText('users.csv')\n",
        "      | 'format line' >> beam.ParDo(Transform())\n",
        "  )\n",
        "\n",
        "  # let's get that global total\n",
        "  totals = cleaned_csv | 'Count elements' >> beam.combiners.Count.Globally()\n",
        "\n",
        "  # now,\n",
        "  counts = ( cleaned_csv\n",
        "            | 'add_key' >> beam.Map(lambda elem: (elem[1], 1))  # emit (M/F, 1) pairs\n",
        "            | 'sum' >> beam.CombinePerKey(sum)\n",
        "           )\n",
        "\n",
        "#          --- totals ---\n",
        "#        /                \\\n",
        "# csv ---                    --- flatten\n",
        "#        \\                /\n",
        "#          --- counts ---\n",
        "#\n",
        "# make our two split pipelines converge again with Flatten, and print the values\n",
        "\n",
        "  ((counts, totals)\n",
        "    | 'flatten ' >> beam.Flatten()\n",
        "    | 'Print result' >> beam.Map(print)\n",
        "  )\n"
      ],
      "metadata": {
        "id": "mIlJA-6EQ0kD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete it with a single beam pipeline that gives us the final answer directly, we co-opted the example [Example 8: Combining with a CombineFn](https://beam.apache.org/documentation/transforms/python/aggregation/combinevalues/). We copied and pasted the code exactly and then just had to make sure that the pipeline was as expected.\n",
        "\n",
        "In their example, they took tuples `('label': [list])`, and computed percentages\n",
        "```\n",
        "('spring', ['ðŸ¥•', 'ðŸ…', 'ðŸ¥•', 'ðŸ…', 'ðŸ†']),\n",
        "('summer', ['ðŸ¥•', 'ðŸ…', 'ðŸŒ½', 'ðŸ…', 'ðŸ…']),\n",
        "('fall', ['ðŸ¥•', 'ðŸ¥•', 'ðŸ…', 'ðŸ…']),\n",
        "('winter', ['ðŸ†', 'ðŸ†'])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "LEXTqO8sQ4HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# this custom combiner was taken directly from beam's documentation:\n",
        "###\n",
        "# BEGIN: COPY PASTA\n",
        "###\n",
        "class AverageFn(beam.CombineFn):\n",
        "  def create_accumulator(self):\n",
        "    return {}\n",
        "\n",
        "  def add_input(self, accumulator, input):\n",
        "    if input not in accumulator:\n",
        "      accumulator[input] = 0\n",
        "    accumulator[input] += 1\n",
        "    return accumulator\n",
        "\n",
        "  def merge_accumulators(self, accumulators):\n",
        "    merged = {}\n",
        "    for accum in accumulators:\n",
        "      for item, count in accum.items():\n",
        "        if item not in merged:\n",
        "          merged[item] = 0\n",
        "        merged[item] += count\n",
        "    return merged\n",
        "\n",
        "  def extract_output(self, accumulator):\n",
        "    total = sum(accumulator.values())\n",
        "    percentages = {item: count / total for item, count in accumulator.items()}\n",
        "    return percentages\n",
        "###\n",
        "# END: COPY PASTA\n",
        "###\n",
        "\n",
        "class Transform(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    row_list =  element.split(',')\n",
        "    if len(set(user_header).symmetric_difference(set(row_list))) == 0:\n",
        "      pass\n",
        "    else:\n",
        "      yield ('gender', row_list[1])\n",
        "\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "\n",
        "  csv = (pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText('users.csv')\n",
        "      | 'format line' >> beam.ParDo(Transform())\n",
        "      | 'group' >> beam.GroupByKey()\n",
        "      | 'sum' >> beam.CombineValues(AverageFn())\n",
        "      | 'print' >> beam.Map(print)\n",
        "  )\n",
        "\n"
      ],
      "metadata": {
        "id": "m1LzAIMXQ-vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.2\n",
        "\n",
        "> Perform a transform that counts the number of customers that joined on each day\n",
        "\n",
        "This answer will be the same as simply grouping by dates and counting occurences of the lines (if the customers can't join more than once).\n",
        "\n",
        "Spoiler alert. They are the same."
      ],
      "metadata": {
        "id": "-S2IYmdHOhAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class GetState(beam.DoFn):\n",
        "\n",
        "#   def process(self, x):\n",
        "#     yield x[3].split(',')[1]\n",
        "\n",
        "class Count(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    yield (element[0], len(element[1]))\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText('users.csv')\n",
        "      | 'inline-transform' >> beam.Map(lambda x: x.split(','))\n",
        "      | 'get-day-cust' >> beam.Map(lambda x: (x[4].replace('/', '-'), x[0]))\n",
        "      | 'drop-header' >> beam.Filter(lambda x: x[0] != 'Date joined')\n",
        "      | 'groupby' >> beam.GroupByKey()\n",
        "      | 'count' >> beam.ParDo(Count())\n",
        "      | \"print\" >> beam.Map(print)\n",
        "  )"
      ],
      "metadata": {
        "id": "jJ8LWQKPRH5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.3\n",
        "\n",
        "> Perform a transform that counts the number of customers for each unique state\n"
      ],
      "metadata": {
        "id": "LyA-3r_nOk0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Count(beam.DoFn):\n",
        "\n",
        "  def process(self, element):\n",
        "    yield (element[0], len(element[1]))\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "  (pipeline\n",
        "      | 'Read lines' >> beam.io.ReadFromText('users.csv')\n",
        "      | 'inline-transform' >> beam.Map(lambda x: x.split(','))\n",
        "      | 'drop header' >> beam.Filter(lambda x: 'Date joined' not in x)\n",
        "      | 'get-state-cust' >> beam.Map(lambda x: (x[3].split('-')[1], x[0]))\n",
        "      | 'groupby' >> beam.GroupByKey()\n",
        "      | 'sum' >> beam.ParDo(Count())\n",
        "      | \"print\" >> beam.Map(print)\n",
        "  )"
      ],
      "metadata": {
        "id": "aaBN4Yj7O1r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ER5hFdDRRvC-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}